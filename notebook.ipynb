{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Iterator\n",
    "\n",
    "from src.utils import get_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_palindrome(words: list[str]) -> bool:\n",
    "    \"\"\"Check if word sequence forms palindrome\"\"\"\n",
    "    s = \"\".join(words)\n",
    "    return s == s[::-1]\n",
    "\n",
    "\n",
    "def find_palindrome_centers(words: list[str]) -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Find all potential palindrome centers in words, excluding trivial cases.\n",
    "    Returns list of (word, position) tuples where position could be center of palindrome.\n",
    "    Only returns positions where there's a true palindrome opportunity (entire left substring\n",
    "    matches reverse of right substring).\n",
    "\n",
    "    Time complexity: O(N * L) where N is number of words and L is average word length\n",
    "    Space complexity: O(K) where K is number of valid palindrome centers found\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for word in words:\n",
    "        length = len(word)\n",
    "        if length < 3:  # Skip very short words\n",
    "            continue\n",
    "\n",
    "        # For each position (excluding first two and last two characters)\n",
    "        for pos in range(2, length - 2):\n",
    "            # Skip center position of word\n",
    "            if pos == length // 2:\n",
    "                continue\n",
    "\n",
    "            # Get entire left and right substrings\n",
    "            left = word[:pos]\n",
    "            right = word[pos + 1 :]\n",
    "\n",
    "            # Check if left matches reverse of right (up to shorter length)\n",
    "            min_length = min(len(left), len(right))\n",
    "            if min_length > 0 and left[-min_length:] == right[:min_length][::-1]:\n",
    "                results.append((word, pos))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PalindromeFinder:\n",
    "    vocabulary: set[str] = field(default_factory=set)\n",
    "    prefix_cache: dict[str, set[str]] = field(default_factory=dict)\n",
    "    suffix_cache: dict[str, set[str]] = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize caches after instance creation\"\"\"\n",
    "        self._build_caches()\n",
    "\n",
    "    def _build_caches(self) -> None:\n",
    "        \"\"\"Build prefix and suffix caches for efficient word lookup\"\"\"\n",
    "        for word in self.vocabulary:\n",
    "            for i in range(1, len(word) + 1):\n",
    "                prefix, suffix = word[:i], word[-i:]\n",
    "                self.prefix_cache.setdefault(prefix, set()).add(word)\n",
    "                self.suffix_cache.setdefault(suffix, set()).add(word)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_palindrome(words: list[str]) -> bool:\n",
    "        \"\"\"Check if word sequence forms palindrome\"\"\"\n",
    "        s = \"\".join(words)\n",
    "        return s == s[::-1]\n",
    "\n",
    "    def find_matches(self, pattern: str, match_start: bool = True) -> set[str]:\n",
    "        \"\"\"Find all words that start/end with pattern\"\"\"\n",
    "        return (\n",
    "            self.prefix_cache.get(pattern, set())\n",
    "            if match_start\n",
    "            else self.suffix_cache.get(pattern, set())\n",
    "        )\n",
    "\n",
    "    def find_mismatch(self, words: list[str], center_pos: int) -> tuple[str, bool]:\n",
    "        \"\"\"\n",
    "        Find what needs to be matched and on which side.\n",
    "        Returns (unmatched_portion, needs_right_match)\n",
    "        \"\"\"\n",
    "        s = \"\".join(words)\n",
    "        left, right = s[:center_pos], s[center_pos + 1 :]\n",
    "\n",
    "        # Find length of matching portion\n",
    "        match_len = 0\n",
    "        for i in range(min(len(left), len(right))):\n",
    "            if left[-(i + 1)] != right[i]:\n",
    "                break\n",
    "            match_len = i + 1\n",
    "\n",
    "        # Get unmatched portions\n",
    "        left_unmatched = left[:-match_len] if match_len else left\n",
    "        right_unmatched = right[match_len:] if match_len else right\n",
    "\n",
    "        # Return longer unmatched portion and whether we need to match on right\n",
    "        return (\n",
    "            (left_unmatched, True)\n",
    "            if len(left_unmatched) >= len(right_unmatched)\n",
    "            else (right_unmatched, False)\n",
    "        )\n",
    "\n",
    "    def grow_palindromes(\n",
    "        self, words: list[str], center_pos: int, depth: int = 5\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Recursively grow palindromes from initial words.\n",
    "        Yields valid palindromes as space-separated strings.\n",
    "        \"\"\"\n",
    "        if depth <= 0:\n",
    "            return\n",
    "\n",
    "        if self.is_palindrome(words):\n",
    "            yield \" \".join(words)\n",
    "            return\n",
    "\n",
    "        mismatch, needs_right = self.find_mismatch(words, center_pos)\n",
    "        if not mismatch:\n",
    "            return\n",
    "\n",
    "        # Find matching words for the reversed mismatch pattern\n",
    "        pattern = mismatch[::-1]\n",
    "        matches = self.find_matches(pattern, match_start=needs_right)\n",
    "\n",
    "        for word in matches:\n",
    "            new_words = words + [word] if needs_right else [word] + words\n",
    "            new_center = center_pos if needs_right else center_pos + len(word)\n",
    "\n",
    "            if self.is_palindrome(new_words):\n",
    "                yield \" \".join(new_words)\n",
    "            yield from self.grow_palindromes(new_words, new_center, depth - 1)\n",
    "\n",
    "\n",
    "def filter_palindrome(palindrome: str, min_avg_length: int = 5) -> bool:\n",
    "    \"\"\"\n",
    "    Filter palindromes based on criteria:\n",
    "    - No repeated words\n",
    "    - Minimum average word length\n",
    "    - First word doesn't contain palindrome prefix\n",
    "    \"\"\"\n",
    "    words = palindrome.split()\n",
    "\n",
    "    # Check for repeated words\n",
    "    if len(set(words)) != len(words):\n",
    "        return False\n",
    "\n",
    "    # Check average word length\n",
    "    if sum(len(word) for word in words) / len(words) < min_avg_length:\n",
    "        return False\n",
    "\n",
    "    # Check first word for palindrome prefix\n",
    "    first_word = words[0]\n",
    "    return not any(\n",
    "        PalindromeFinder.is_palindrome([first_word[:i]]) for i in range(2, 7)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(top_n=50000)\n",
    "print(vocabulary)\n",
    "\n",
    "results = find_palindrome_centers(vocabulary)\n",
    "print(results)\n",
    "print(f\"{len(results)} potential palindrome centers: {results[:5]}...\")\n",
    "\n",
    "finder = PalindromeFinder(vocabulary)\n",
    "print(\"Finding palindromes...\")\n",
    "\n",
    "for palindrome in finder.grow_palindromes([\"be\"], center_pos=0, depth=6):\n",
    "    if filter_palindrome(palindrome):\n",
    "        print(palindrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1506OVHAzm7jIleB7oHGoSbuQuytaFWvu#scrollTo=ewd0Ky7VOhqq\n",
    "https:// github.com/ML-GSAI/LLaDA/tree/main\n",
    "https://ml-gsai.github.io/LLaDA-demo/\n",
    "https://huggingface.co/GSAI-ML/LLaDA-8B-Base\n",
    "https://github.com/hamishivi/tess-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import F\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n",
      "- configuration_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n",
      "- modeling_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards: 100%|██████████| 6/6 [03:16<00:00, 32.71s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model_name = \"GSAI-ML/LLaDA-8B-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "print(\"Model config:\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi!\"\n",
    "num_steps = 1\n",
    "print(f\"\\nGenerating from prompt: '{prompt}'\")\n",
    "\n",
    "# Tokenize\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Based on convert_to_simplex from sdlm/utils.py\n",
    "# This creates a one-hot encoding and scales it\n",
    "simplex_value = 5.0  # From SDLM code examples\n",
    "vocab_size = model.config.vocab_size\n",
    "simplex = 2 * simplex_value * torch.nn.functional.one_hot(input_ids, vocab_size).float() - simplex_value\n",
    "simplex = simplex.to(device)\n",
    "print(f\"Simplex shape: {simplex.shape}\")\n",
    "\n",
    "# Create a span mask (what tokens to generate)\n",
    "# In SDLM, this would indicate which tokens to diffuse/generate\n",
    "# True = positions to generate, False = fixed input\n",
    "span_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "prefix_len = input_ids.shape[1]  # Use input as prefix\n",
    "\n",
    "# Let's add some positions for generated text\n",
    "max_new_tokens = 10\n",
    "# Extend input_ids, simplex and span_mask\n",
    "padding = torch.zeros(1, max_new_tokens, dtype=input_ids.dtype).to(device)\n",
    "full_input_ids = torch.cat([input_ids, padding], dim=1)\n",
    "\n",
    "# Extend simplex with random noise for new positions\n",
    "noise_shape = (1, max_new_tokens, vocab_size)\n",
    "random_simplex = (torch.randn(noise_shape).to(device) * simplex_value)\n",
    "full_simplex = torch.cat([simplex, random_simplex], dim=1)\n",
    "\n",
    "# Set span mask: False for input, True for positions to generate\n",
    "full_span_mask = torch.zeros_like(full_input_ids, dtype=torch.bool)\n",
    "full_span_mask[:, prefix_len:] = True\n",
    "\n",
    "print(f\"Full input shape: {full_input_ids.shape}\")\n",
    "print(f\"Full simplex shape: {full_simplex.shape}\")\n",
    "print(f\"Full span mask shape: {full_span_mask.shape}\")\n",
    "print(f\"Positions to generate (span_mask=True): {full_span_mask.sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Start diffusion process - from high noise to low noise\n",
    "for step in range(num_steps, -1, -1):\n",
    "    # Calculate timestep (noise level)\n",
    "    timestep = torch.ones_like(full_input_ids).float() * (step / num_steps)\n",
    "    timestep = timestep.to(device)\n",
    "    print(f\"\\nStep {num_steps-step}/{num_steps}: Timestep={timestep[0, 0].item():.2f}\")\n",
    "\n",
    "    # In SDLM, there would be a self-conditioning step here using previous predictions\n",
    "    # For simplicity, we'll skip that\n",
    "\n",
    "    # Forward pass through model\n",
    "    with torch.no_grad():\n",
    "        # Similar to warp_timesteps in sdlm models\n",
    "        print(f\"  Running model inference...\")\n",
    "\n",
    "        # We'd need the actual timestep embedding for full implementation\n",
    "        # For now, we'll assume the model knows what to do with the timestep\n",
    "\n",
    "        # Turn simplex values into probabilities with softmax\n",
    "        probs = torch.nn.functional.softmax(full_simplex, dim=-1)\n",
    "\n",
    "        # Similar to model.vocab_to_hidden_dim_embed in SDLM\n",
    "        inputs_embeds = torch.matmul(probs, model.get_input_embeddings().weight)\n",
    "\n",
    "        # For actual TESS2, we'd need proper timestep embedding\n",
    "        # Here we do a simple approach\n",
    "        timestep_embed = timestep.unsqueeze(-1) * 0.1  # Simple scaling\n",
    "\n",
    "        # Apply span mask - use original word embeddings for input tokens\n",
    "        original_embeds = model.get_input_embeddings()(full_input_ids)\n",
    "\n",
    "        # Combine embeddings based on span mask\n",
    "        combined_embeds = torch.where(\n",
    "            full_span_mask.unsqueeze(-1),\n",
    "            inputs_embeds + timestep_embed,\n",
    "            original_embeds\n",
    "        )\n",
    "\n",
    "        # Run model\n",
    "        outputs = model(inputs_embeds=combined_embeds)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Project back to vocabulary space\n",
    "        logits = torch.matmul(hidden_states, model.get_input_embeddings().weight.transpose(0, 1))\n",
    "\n",
    "        # Sample from logits (from sdlm/inference/inference_utils.py)\n",
    "        selected_tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Update the simplex values for generated positions\n",
    "        new_simplex = 2 * simplex_value * torch.nn.functional.one_hot(\n",
    "            selected_tokens, vocab_size\n",
    "        ).float() - simplex_value\n",
    "\n",
    "        # Only update the positions we're generating\n",
    "        full_simplex = torch.where(\n",
    "            full_span_mask.unsqueeze(-1),\n",
    "            (1 - 0.1) * full_simplex + 0.1 * new_simplex,  # Simple update rule\n",
    "            full_simplex\n",
    "        )\n",
    "\n",
    "        # Update the input_ids with newly selected tokens\n",
    "        full_input_ids = torch.where(\n",
    "            full_span_mask,\n",
    "            selected_tokens,\n",
    "            full_input_ids\n",
    "        )\n",
    "\n",
    "        # Print current state\n",
    "        generated_text = tokenizer.decode(full_input_ids[0], skip_special_tokens=True)\n",
    "        print(f\"  Generated so far: {generated_text}\")\n",
    "        results.append(generated_text)\n",
    "\n",
    "print(\"\\nFinal generation:\", results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
